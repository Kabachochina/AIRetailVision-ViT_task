21.03.2025 (пятница):
Начало работы над проектом.
- загрузил датасет с kaggle
- разделил категории на
лицензируемые: baby_products, beauty_health, electronics, grocery, pet_supplies
и нелицензируемые: clothing_accessories_jewellery, hobby_arts_stationary, home_kitchen_tools, sports_outdoor
- создал кастомный датасет CustomDataset для работы с изображениями
метка 0 обозначает, что товар нелицензируем, 1 - лицензируем
- начал изучать, что такое vision transformer.
https://arxiv.org/pdf/2006.03677 - статья по vit

22.03.2025 (суббота):
Продолжаю разбираться с vision transformer
просмотрел некоторые материалы про attention, transformer
наткнулся на статью https://arxiv.org/pdf/2010.11929
(AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE)

23.03.2025 (воскресенье):
начал работу над созданием vit модели

25.03.2025 (вторник):
написал классификатор на основе ViT. Буду тестировать его

26.03.2025(среда):
написал методы для тренировки и тестирования модели
написал методы для сохранения модели на диск и загрузки сохранённой модели

27.03.2025(Четверг):
Попробовал обучить модель, но на тесте показывала accuracy > 100%.
Дебажу и пытаюсь найти ошибку в реализации
Исправил ошибку. Она была в подсчёте correct в test_model

30.03.2025(воскресенье):
Теперь вместо обучения своей модели с нуля, буду заниматься fine tuning-ом
Знакомство с библиотекой transformers от hugging face
изменил класс custom_dataset, убрав transform и добавив processor: ViTImageProcessor
для использования подходящих трансформаций изображений перед подачей в предобученную модель
Пробую произвести дообучение модели google/vit-base-patch16-224-in21k

31.03.2025(понедельник):
дообучил модель и получил точность (accuracy) 92.32% на тестовой выборке (преобразованной с помощью
processor, который использовался при обучении) и 91.14% при преобразовании ToTensor()
модель сохранил в saved_model
saved_model запушить не получилось (много весит) :(
код для тренировки модели в src/train_vit.py
Приступим к проведению атаки. Начну с PGD атаки.
Реализовал класс для PGD атаки в attack.py
Подбираю гиперпараметры для pgd атаки "на глаз". Чтобы не слишком заметно было человеческому глазу
первые подобранные гиперпараметры:
    epsilon=0.01,
    steps=10,
    step_size=0.001

01.04.2025(вторник):
написал функцию attack_pgd_and_save_images для проведения pgd атаки на изображения
и сохранения картинок на жёсткий диск. Тестирование модели на этих изображениях даёт Test Accuracy: 59.06%
Написал функцию pgd_test_model для атаки изображений в режиме тестирования on-th-fly.
При тех же гиперпараметрах атаки, что и в случае с attack_pgd_and_save_images у меня получается другая точность
предсказаний, а именно Test Accuracy: 17.52%
Главная проблема оказалась в формате изображения в котором я сохранял (jpeg). Во время сжатия всё портилось
Изменил на png и точность на сохранённых (атакованных) картинках стала около 6%
Затем изменил способ сохранения картинки plt.imsave -> pil_image.save и точность на сохранённых
атакованных картинках стала 16.73%, что слабо отличается от 17.52%, полученных методом on-the-fly.
Довольно примечательно получилось :)
Также интересно, что при достаточно (на мой взгляд) малых возмущениях модель так упала в точности.
К тому же на глаз (не экспертный конечно, но всё же) разницы в атакованном изображении и обычном никакой

Начал знакомство с библиотекой Captum для интерпретации. Реализовал метод integrates gradients
в ноутбуке interpretability_analys.ipynb. Пока, честно говоря, не очень понятно, на что модель обращает внимание,
но есть предположение, что в большей степени на контуры объектов на изображении. Также (по моим наблюдениям) модель
не обращает внимания на тёмные области в изображении.

02.04.2025(среда):
Пробую использовать другие методы интерпретации из библиотеки Captum. Пока почему то вся видеопамять мгновенно заканчивается
и выходит out of memory...
Пробую найти ещё методы интерпретации визуальных трансформеров.
Реализовал интерпретацию весов слоёв внимания модели. Более подробно написано в ноутбуке про интерпретацию.
Также поверхностно познакомился со статьёй Axiomatic Attribution for Deep Networks, чтобы немного лучше понимать, что под
капотом integrated_gradients библиотеки Captum

03.04.2025(четверг):
Сегодня пробую наложить усреднение (по всем головам и слоям) весов внимания на обычное и атакованное изображение

Небольшой итог всего, что я пытался сделать с интерпретацией.
Integrated Gradients слабо меняется при атаке PGD, возможно из-за того, что возмущения достаточно малы.
При визуализации усреднения по головам для каждого слоя, на некоторых слоях можно хорошо рассмотреть разницу
между атакованными и обычными примерами. Но вот если усреднять по всем слоям и накладывать на изображение, то
визуальное различие часто довольно мало. Иногда на тепловой карте при атаке появляется 1-2 точки,
куда раньше модель не обращала особого внимания или внимание усиливается в тех областях, где оно было.

Добавил код тренировки модели в отдельный файл, чтобы не искать его в коммитах

Последним пунктом - спроектировать атаку, которая изменит предсказания модели и останется незаментной для методов
интерпретации. Спроектировать и тем более реализовать уже вряд ли успею, но есть предположение следующее:
допустим, мы хотим защититься от обнаружения каким-либо одним методом интерпретации. Пусть будем защищаться от визуализации
весов внимания. Для этого можно считать разницу матриц весов внимания для оригинального изображения
и для атакованного и вносить изменения так, чтобы эта разница была меньше какого-нибудь эпсилон. При этом нам нужно
максимизировать функцию потерь (чтобы обмануть модель). Получается мы одновременно пытаемся минимизировать
функцию разницы матриц внимания (атакованного и обычного изображения), и также максимизировать функцию потерь для
модели, своими малыми изменениями изображения. И эти изменения не должны разрушить картинку. Мы же не только модель
обманываем, но и людей.
Вот только есть ощущение, что такая задача во-первых может быть не всегда разрешима в принципе
Во-вторых, даже сделав всё возможное и подобрав самые оптимальные возмущения
мы не защищены от каких-нибудь других методов интерпретации.
Вдобавок это достаточно вычислительно сложная задача.

