21.03.2025 (пятница):
Начало работы над проектом.
- загрузил датасет с kaggle
- разделил категории на
лицензируемые: baby_products, beauty_health, electronics, grocery, pet_supplies
и нелицензируемые: clothing_accessories_jewellery, hobby_arts_stationary, home_kitchen_tools, sports_outdoor
- создал кастомный датасет CustomDataset для работы с изображениями
метка 0 обозначает, что товар нелицензируем, 1 - лицензируем
- начал изучать, что такое vision transformer.
https://arxiv.org/pdf/2006.03677 - статья по vit

22.03.2025 (суббота):
Продолжаю разбираться с vision transformer
просмотрел некоторые материалы про attention, transformer
наткнулся на статью https://arxiv.org/pdf/2010.11929
(AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE)

23.03.2025 (воскресенье):
начал работу над созданием vit модели

25.03.2025 (вторник):
написал классификатор на основе ViT. Буду тестировать его

26.03.2025(среда):
написал методы для тренировки и тестирования модели
написал методы для сохранения модели на диск и загрузки сохранённой модели

27.03.2025(Четверг):
Попробовал обучить модель, но на тесте показывала accuracy > 100%.
Дебажу и пытаюсь найти ошибку в реализации
Исправил ошибку. Она была в подсчёте correct в test_model

30.03.2025(воскресенье):
Теперь вместо обучения своей модели с нуля, буду заниматься fine tuning-ом
Знакомство с библиотекой transformers от hugging face
изменил класс custom_dataset, убрав transform и добавив processor: ViTImageProcessor
для использования подходящих трансформаций изображений перед подачей в предобученную модель
Пробую произвести дообучение модели google/vit-base-patch16-224-in21k

31.03.2025(понедельник):
дообучил модель и получил точность (accuracy) 92.32% на тестовой выборке (преобразованной с помощью
processor, который использовался при обучении) и 91.14% при преобразовании ToTensor()
модель сохранил в saved_model
saved_model запушить не получилось (много весит) :(
КОД ИСПОЛЬЗОВАВШИЙСЯ ДЛЯ ПРОВЕДЕНИЯ ТРЕНИРОВКИ МОДЕЛИ есть в commit: training model
Приступим к проведению атаки. Начну с PGD атаки.
Реализовал класс для PGD атаки в attack.py
Подбираю гиперпараметры для pgd атаки "на глаз". Чтобы не слишком заметно было человеческому глазу
первые подобранные гиперпараметры:
    epsilon=0.01,
    steps=10,
    step_size=0.001

01.04.2025(вторник):
написал функцию attack_pgd_and_save_images для проведения pgd атаки на изображения
и сохранения картинок на жёсткий диск. Тестирование модели на этих изображениях даёт Test Accuracy: 59.06%
Написал функцию pgd_test_model для атаки изображений в режиме тестирования on-th-fly.
При тех же гиперпараметрах атаки, что и в случае с attack_pgd_and_save_images у меня получается другая точность
предсказаний, а именно Test Accuracy: 17.52%
Главная проблема оказалась в формате изображения в котором я сохранял (jpeg). Во время сжатия всё портилось
Изменил на png и точность на сохранённых (атакованных) картинках стала около 6%
Затем изменил способ сохранения картинки plt.imsave -> pil_image.save и точность на сохранённых
атакованных картинках стала 16.73%, что слабо отличается от 17.52%, полученных методом on-the-fly.
Довольно примечательно получилось :)
Также интересно, что при достаточно (на мой взгляд) малых возмущениях модель так упала в точности.
К тому же на глаз (не экспертный конечно, но всё же) разницы в атакованном изображении и обычном никакой

Начал знакомство с библиотекой Captum для интерпретации. Реализовал метод integrates gradients
в ноутбуке interpretability_analys.ipynb. Пока, честно говоря, не очень понятно, на что модель обращает внимание,
но есть предположение, что в большей степени на контуры объектов на изображении. Также (по моим наблюдениям) модель
не обращает внимания на тёмные области в изображении.

